#!/usr/bin/env python2.7

import argparse, subprocess, os, gzip, logging, time, collections, HTSeq
from itertools import izip_longest

logging.basicConfig(filename='spliceSeq.log', filemode='w', format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO)

def main(args):

	global_start_time = time.time()
	samples = []

	logging.info('Reading intron ranges from %s' % (args.intron_intervals))
	introns = HTSeq.GenomicArrayOfSets("auto", stranded=False)
	for line in open(args.intron_intervals):
		fields = line.rstrip().split('\t')
		iv = HTSeq.GenomicInterval(fields[1], int(fields[2])-1, int(fields[3]))
		introns[iv] += fields[0]

	for n, i in enumerate(args.input_files):

		sample_start_time = time.time()

		infile = i.split('/')[-1]
		logging.info(' Processing %s' % (infile))
		
		samples.append(infile)

		# Create folder for each sample
		if not os.path.exists(infile):
			os.makedirs(infile)
		
		# Extract Molecular Counter and Remove PCR Duplicates
		if not args.skipPCRMC:
			logging.info('\tRemoving PCR Duplicates and Extracting Molecular Counter')
			removePCRDuplicates_ExtractMolecularCounter(i, infile, args.moleCountLen)
		else:
			logging.info('\tSkipping Removal of PCR Duplicates and Extracting Molecular Counter')

		# Trim adapter sequences
		if not args.skipTrim:
			logging.info('\tTrimming Adapter Sequences')
			trimAdapters(infile, args.trimJar, args.trimAdapter)
		else:
			logging.info('\tSkipping Trimming of Adapter Sequences')

		# Alignment
		if not args.skipAlignment:
			logging.info('\tAligning Reads to Genome')
			align(infile, args.hisatIndex)
		else:
			logging.info('\tSkipping Alignment of Reads to Genome')

		# Pool Reads
		if not args.skipPool:
			logging.info('\tPooling Reads')
			pool(infile, introns, args.junction_anchor)
		else:
			logging.info('\tSkipping Pooling of Reads')


	logging.info('Total Time: %ds' % (time.time()-global_start_time))

def removePCRDuplicates_ExtractMolecularCounter(full_path, infile, n):

	occured = set()
	numProcessed = 0
	numUnique = 0
	out1 = []
	out2 = []
	local_start_time = time.time()

	with gzip.open('%s_R1.fastq.gz' % (full_path)) as i1, gzip.open('%s_R2.fastq.gz' % (full_path)) as i2:
		for line1, line2 in izip_longest(i1, i2):
			
			info1 = line1.rstrip().split(' ')[0]
			info2 = line2.rstrip().split(' ')[0]

			seq1 = next(i1).rstrip()
			extra1 = next(i1).rstrip()
			quality1 = next(i1).rstrip()

			seq2 = next(i2).rstrip()
			extra2 = next(i2).rstrip()
			quality2 = next(i2).rstrip()
 
			if info1 != info2:
				print 'Error: %s does not match %s' % (info1, info2)

			if (seq1, seq2) not in occured:
				out1.append('%s;%s\n%s\n%s\n%s\n' % (info1, seq1[:n], seq1[n:], extra1, quality1[n:]))
				out2.append('%s;%s\n%s\n%s\n%s\n' % (info2, seq1[:n], seq2, extra2, quality2))
				numUnique += 1
				occured.add((seq1, seq2))

			if numProcessed % 1000000 == 0:
				logging.info('\t\tProcessed %d Reads in %ds' % (numProcessed, time.time()-local_start_time))

			numProcessed += 1

	logging.info('\t\tProcessed %d Total Reads in %ds' % (numProcessed, time.time()-local_start_time))

	logging.info('\t\tWriting Processed Read 1')
	start_time = time.time()
	with gzip.open('%s/%s_R1_dupMole.fastq.gz' % (infile, infile) ,'wb') as out:
		out.write(''.join(out1))
	logging.info('\t\t\tWrote %d reads in %ds' % (numUnique, time.time()-start_time))

	logging.info('\t\tWriting Processed Read 2')
	start_time = time.time()
	with gzip.open('%s/%s_R2_dupMole.fastq.gz' % (infile, infile) , 'wb') as out:
		out.write(''.join(out2))
	logging.info('\t\t\tWrote %d reads in %ds' % (numUnique, time.time()-start_time))

def trimAdapters(infile, trimmomatic_jar, trimmomatic_adapters):
	cmd = 'java -jar %s PE -phred33 %s/%s_R1_dupMole.fastq.gz %s/%s_R2_dupMole.fastq.gz %s/%s_R1_P.fastq.gz %s/%s_R1_U.fastq.gz %s/%s_R2_P.fastq.gz %s/%s_R2_U.fastq.gz ILLUMINACLIP:%s:2:30:10 MINLEN:5' % (trimmomatic_jar, infile, infile, infile, infile, infile, infile, infile, infile, infile, infile, infile, infile, trimmomatic_adapters)
	proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
	(out, err) = proc.communicate()
	with open('%s/%s_trim.log' % (infile, infile), 'w') as trimOut:
		trimOut.write(err)
	info = err.split('\n')[9].split(' ')
	logging.info('\t\tInput Read Pairs: %s\n\t\t\t\tBoth Surviving: %s\n\t\t\t\tForward Only Surviving: %s\n\t\t\t\tReverse Only Surviving: %s\n\t\t\t\tDropped: %s' % (info[3], info[6], info[11], info[16], info[19]))

def align(infile, index):
	cmd = 'hisat2 -p 4 --phred33 --max-intronlen 1000 -x %s -1 %s/%s_R1_P.fastq.gz -2 %s/%s_R2_P.fastq.gz | samtools view -bh -q 5 - | samtools sort - -o %s/%s.bam' % (index, infile, infile, infile, infile, infile, infile)
	proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
	(out, err) = proc.communicate()
	with open('%s/%s_alignment.log' % (infile, infile), 'w') as alignOut:
		alignOut.write(err)

def pool(infile, introns, junction_anchor):
	intron_counts = collections.Counter()
	junction_counts = collections.Counter()

	ambiguous_introns = 0
	ambiguous_junctions = 0

	numProcessed = 0
	local_start_time = time.time()

	logging.info('\t\tProcessing Alignment Fragments')
	for first, second in HTSeq.pair_SAM_alignments_with_buffer(HTSeq.BAM_Reader('%s/%s.bam' % (infile, infile))):
		intron_ids = set()
		junction_ids = set()
					potential_junction_ids = set()
		if first != None and first.aligned and first.aQual > 5:
			pre_match = False
			junction_occured = False
			for cigop in first.cigar:
				if cigop.type == 'M':
					if not junction_occured:
						pre_match = cigop.size > junction_anchor
					else:
						if pre_match and cigop.size > junction_anchor:
							junction_ids |= potential_junction_ids
							pre_match = cigop.size > junction_anchor
							junction_occured = False
					for iv, val in introns[cigop.ref_iv].steps():
						intron_ids |= val
				elif cigop.type == 'N':
					junction_occured = True
					for iv, val in introns[cigop.ref_iv].steps():
						potential_junction_ids |= val
		if second != None and second.aligned and second.aQual > 5:
			pre_match = False
			junction_occured = False
			for cigop in second.cigar:
				if cigop.type == 'M':
					if not junction_occured:
						pre_match = cigop.size > junction_anchor
					else:
						if pre_match and cigop.size > junction_anchor:
							junction_ids |= potential_junction_ids
							pre_match = cigop.size > junction_anchor
							junction_occured = False
					for iv, val in introns[cigop.ref_iv].steps():
						intron_ids |= val
				elif cigop.type == 'N':
					junction_occured = True
					for iv, val in introns[cigop.ref_iv].steps():
						potential_junction_ids |= val

		candidateGenes = set()
		for i in intron_ids:
			candidateGenes.add(i.split(';')[0])
		for i in junction_ids:
			candidateGenes.add(i.split(';')[0])

		if len(candidateGenes) == 1:
			for intron_id in list(intron_ids):
				intron_counts[intron_id] += 1
		elif len(candidateGenes) > 1:
			ambiguous_introns += 1

		if len(candidateGenes) == 1:
			for junction_id in list(junction_ids):
				junction_counts[junction_id] += 1
		elif len(junction_ids) > 1:
			ambiguous_junctions += 1

		if numProcessed > 0 and numProcessed % 1000000 == 0:
			logging.info('\t\tProcessed %d Fragments in %ds' % (numProcessed, time.time()-local_start_time))
		numProcessed += 1

	logging.info('\t\tProcessed %d Fragments in %ds' % (numProcessed, time.time()-local_start_time))

	logging.info('\tCalculating Splicing Information')
	combined = set()
	for feature in intron_counts.keys():
		combined.add(feature)
	for feature in junction_counts.keys():
		combined.add(feature)

	with open('%s/%s_splicingCounts.txt' % (infile, infile), 'w') as splicingOut:
		for feature in sorted(combined):
			splicingOut.write('%s\t%d\t%d\n' % (feature, junction_counts[feature], intron_counts[feature]))

def parseArguments():
	parser = argparse.ArgumentParser(prog="MPE_PipeLine_PE", description='', usage='%(prog)s [options]')
	required = parser.add_argument_group('required arguments')
	dup_mole = parser.add_argument_group('Extracting Molecular Counter / PCR Duplicate Removal Options')
	trim = parser.add_argument_group('Trimming Options')
	alignment = parser.add_argument_group('Alignment Options')
	pool = parser.add_argument_group('Pooling Options')
	required.add_argument('-i', '--input_files', nargs='+', required=True, help=' Basename of files to run. (fastq.gz)', metavar='', dest='input_files')
	dup_mole.add_argument('-m', '--molecularCounterLen', type=int, default=7, help=" Length of Molecular Counter at 5' end of Read 1. (Default: 7)", metavar='', dest='moleCountLen')
	dup_mole.add_argument('--skip_PCRMC', action='store_true', help=' Skip the removal of PCR duplicates and extraction of the molecular counter. Assumes files already exist.', dest='skipPCRMC')
	trim.add_argument('--skip_Trim', action='store_true', help=' Skip the trimming of adapter sequences. Assumes files already exist.', dest='skipTrim')
	trim.add_argument('--trimmomatic_path', default='/opt/Trimmomatic-0.36/trimmomatic-0.36.jar', help=' Path to Trimmomatic jar file.', dest='trimJar')
	trim.add_argument('--trimmomatic_jar', default='/opt/Trimmomatic-0.36/adapters/NexteraPE-PE.fa', help=' Path to adapter file.', dest='trimAdapter')
	alignment.add_argument('--skip_Align', action='store_true', help=' Skip the alignment of reads to genome. Assumes files already exist.', dest='skipAlignment')
	alignment.add_argument('--hisat_index', default='~/Lab/Genomes/pombe/hisat/sp_hisatIndex_v2.30', help=' Location of hisat index files.', dest='hisatIndex')
	pool.add_argument('--skip_Pool', action='store_true', help=' Skip the pooling of reads. Assumes files already exist.', dest='skipPool')
	pool.add_argument('--intron_intervals', default='/home/zdwyer/Lab/Genomes/pombe/sp_intronRanges_v2.30.txt', help=' File containing intervals for which to count reads.', dest='intron_intervals')
	pool.add_argument('--junction_anchor', default=3, type=int, help=' Required length of anchor on each side of junction.', dest='junction_anchor')
	return parser.parse_args()

args = parseArguments()
main(args)
